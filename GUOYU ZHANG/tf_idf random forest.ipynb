{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0",
      "cell_type": "markdown",
      "source": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Load phishing emails and normal emails data\nphishing_file_path = '/Users/zhangguoyu/Downloads/CaptstoneProjectData_2024.csv'\nphishing_data = pd.read_csv(phishing_file_path)\n\nnormal_file_path = '/Users/zhangguoyu/Downloads/emails.csv'\nnormal_data = pd.read_csv(normal_file_path)\n\n# Fill missing values\nphishing_data['Subject'] = phishing_data['Subject'].fillna('')\nphishing_data['Body'] = phishing_data['Body'].fillna('')\nnormal_data['file'] = normal_data['file'].fillna('')\nnormal_data['message'] = normal_data['message'].fillna('')\n\n# Simple text preprocessing function\ndef simple_preprocess_text(text):\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\d', ' ', text)\n    text = text.lower()\n    text = text.replace('________________________________', '')\n    words = text.split()\n    stop_words = {\n        'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n        'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could',\n        \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for',\n        'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\",\n        'her', 'here', \"here's\", 'hers', 'herself', 'him', \"himself\", 'his', 'how', \"how's\", 'I', \"I'd\", \"I'll\", \"I'm\",\n        \"I've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', 'let', \"let's\", 'me', 'more', 'most',\n        \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our',\n        'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n        \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n        'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n        'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were',\n        \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom',\n        'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n        'yourself', 'yourselves'\n    }\n    words = [word for word in words if word not in stop_words]\n    return ' '.join(words)\n\n# Apply preprocessing function\nphishing_data['Cleaned_Subject'] = phishing_data['Subject'].apply(simple_preprocess_text)\nphishing_data['Cleaned_Body'] = phishing_data['Body'].apply(simple_preprocess_text)\nnormal_data['Cleaned_Subject'] = normal_data['file'].apply(simple_preprocess_text)\nnormal_data['Cleaned_Body'] = normal_data['message'].apply(simple_preprocess_text)\n\n# Combine cleaned subject and body\nphishing_data['Cleaned_Text'] = phishing_data['Cleaned_Subject'] + \" \" + phishing_data['Cleaned_Body']\nnormal_data['Cleaned_Text'] = normal_data['Cleaned_Subject'] + \" \" + normal_data['Cleaned_Body']\n\n# Add labels\nphishing_data['Label'] = 1\nnormal_data['Label'] = 0\n\n# Combine datasets\nall_emails = pd.concat([phishing_data, normal_data], ignore_index=True)\n\n# Initialize TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)\n\n# Extract TF-IDF features\ntfidf_matrix = tfidf_vectorizer.fit_transform(all_emails['Cleaned_Text'])\n\n# Convert to DataFrame\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\ntfidf_df['Label'] = all_emails['Label'].values\n\n# Split the dataset\nX = tfidf_df.drop('Label', axis=1)\ny = tfidf_df['Label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint('Classification Report:')\nprint(report)\n\n# Save TF-IDF features to CSV file\noutput_file_path = '/Users/zhangguoyu/Downloads/tfidf_features.csv'\ntfidf_df.to_csv(output_file_path, index=False)\n\nprint(f\"TF-IDF features saved to file: {output_file_path}\")",
      "metadata": {}
    },
    {
      "id": "6a9c4fb1-bde6-46de-82da-c51926b8d3cb",
      "cell_type": "code",
      "source": "Accuracy: 1.0\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    103447\n           1       1.00      1.00      1.00       549\n\n    accuracy                           1.00    103996\n   macro avg       1.00      1.00      1.00    103996\nweighted avg       1.00      1.00      1.00    103996",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "332a2bf6-0a17-40dd-bc90-268f941fccac",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c8432cb9-1219-4ec2-af8e-09e89fec6d56",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "712fb603-d499-4c30-898d-69a0c3a949c0",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}