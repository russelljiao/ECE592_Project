{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1441924-8ab1-428d-9275-7a4fd76a754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理后的数据:\n",
      "                                     Cleaned_Subject  \\\n",
      "0  review shipment details shipment notification ...   \n",
      "1                            υоur ассоunt іѕ оn hоld   \n",
      "2               completed invoice kz tys bestbuy com   \n",
      "3                              uvic important notice   \n",
      "4                        suspended incoming messages   \n",
      "\n",
      "                                        Cleaned_Body  \\\n",
      "0  notice message sent outside university victori...   \n",
      "1  votre réponse bien été prise en compte υоur ас...   \n",
      "2  notice message sent outside university victori...   \n",
      "3  uvic account filed list accounts set deactivat...   \n",
      "4  message generated uvic ca source sender action...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  review shipment details shipment notification ...  \n",
      "1  υоur ассоunt іѕ оn hоld votre réponse bien été...  \n",
      "2  completed invoice kz tys bestbuy com notice me...  \n",
      "3  uvic important notice uvic account filed list ...  \n",
      "4  suspended incoming messages message generated ...  \n",
      "N-gram characteristic matrix:\n",
      "   able  access  account  account will  accounts  action  action required  \\\n",
      "0     0       0        0             0         0       1                1   \n",
      "1     0       0        0             0         0       0                0   \n",
      "2     0       2        0             0         0       0                0   \n",
      "3     0       0        4             0         1       0                0   \n",
      "4     0       0        0             0         0       2                1   \n",
      "\n",
      "   active  activity  address  ...  wish  within  within hours  without  work  \\\n",
      "0       0         0        2  ...     0       0             0        0     0   \n",
      "1       0         0        0  ...     0       0             0        0     0   \n",
      "2       0         0        0  ...     0       0             0        0     0   \n",
      "3       1         0        0  ...     0       0             0        0     0   \n",
      "4       0         0        0  ...     0       0             0        0     0   \n",
      "\n",
      "   working  world  year  years  zip  \n",
      "0        0      0     0      0    0  \n",
      "1        0      0     0      0    0  \n",
      "2        0      0     0      2    0  \n",
      "3        0      0     0      0    0  \n",
      "4        0      0     0      0    0  \n",
      "\n",
      "[5 rows x 500 columns]\n",
      "N-gram Features are saved to a file: /Users/jiaoyihan/capstone/capstone_project/Processed_CaptstoneProjectData_2024_ngram.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 加载数据集\n",
    "file_path = '/Users/jiaoyihan/capstone/capstone_project/CaptstoneProjectData_2024.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 填充空值\n",
    "data['Subject'] = data['Subject'].fillna('')\n",
    "data['Body'] = data['Body'].fillna('')\n",
    "\n",
    "# 简单的预处理函数\n",
    "def simple_preprocess_text(text):\n",
    "    # 移除HTML标签\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 移除URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 移除特殊字符和数字\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 移除连续的下划线\n",
    "    text = text.replace('________________________________', '')\n",
    "    # 分词并移除停用词\n",
    "    words = text.split()\n",
    "    stop_words = {\n",
    "        'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    "        'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could',\n",
    "        \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for',\n",
    "        'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\",\n",
    "        'her', 'here', \"here's\", 'hers', 'herself', 'him', \"himself\", 'his', 'how', \"how's\", 'I', \"I'd\", \"I'll\", \"I'm\",\n",
    "        \"I've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', 'let', \"let's\", 'me', 'more', 'most',\n",
    "        \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our',\n",
    "        'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should',\n",
    "        \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "        'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n",
    "        'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were',\n",
    "        \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom',\n",
    "        'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n",
    "        'yourself', 'yourselves'\n",
    "    }\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理函数到Subject和Body列\n",
    "data['Cleaned_Subject'] = data['Subject'].apply(simple_preprocess_text)\n",
    "data['Cleaned_Body'] = data['Body'].apply(simple_preprocess_text)\n",
    "\n",
    "# 合并清理后的Subject和Body文本\n",
    "data['Cleaned_Text'] = data['Cleaned_Subject'] + \" \" + data['Cleaned_Body']\n",
    "\n",
    "# 检查清理后的数据\n",
    "print(\"清理后的数据:\")\n",
    "print(data[['Cleaned_Subject', 'Cleaned_Body', 'Cleaned_Text']].head())\n",
    "\n",
    "# 初始化 N-gram Vectorizer\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=500)  # 限制最多500个特征\n",
    "\n",
    "# 拟合并转换清理后的文本数据\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(data['Cleaned_Text'])\n",
    "\n",
    "# 将 N-gram 矩阵转换为 DataFrame 以便于可视化\n",
    "ngram_df = pd.DataFrame(ngram_matrix.toarray(), columns=ngram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 检查 N-gram DataFrame\n",
    "print(\"N-gram characteristic matrix:\")\n",
    "print(ngram_df.head())\n",
    "\n",
    "# 保存 N-gram 特征到 CSV 文件，不带索引\n",
    "output_file_path = '/Users/jiaoyihan/capstone/capstone_project/Processed_CaptstoneProjectData_2024_ngram.csv'\n",
    "ngram_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"N-gram Features are saved to a file: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8b9c9-d0ad-4f09-bf51-795e327e4016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
